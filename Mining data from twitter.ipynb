{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<twitter.api.Twitter object at 0x000001DA0E11BB88>\n"
     ]
    }
   ],
   "source": [
    "import twitter\n",
    "\n",
    "CONSUMER_KEY = '0FzEgBH0OzgexSwbVmmg8M8fg'\n",
    "CONSUMER_SECRET ='PBMP7U6bOqSMUqQT5f7sT1q5Rj662wvSB4OI2zniol42glMjbC'\n",
    "OAUTH_TOKEN = '1728330055-cv8ZzVBnzEJq9QWUIZ4TKj3VchVF2rprza7h7id'\n",
    "OAUTH_TOKEN_SECRET = 'wLA7l7EPvpjqR1PTc5yzUvA6PikQbY51PbKO25SuKNaVc'\n",
    "\n",
    "auth = twitter.oauth.OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n",
    "                           CONSUMER_KEY, CONSUMER_SECRET)\n",
    "\n",
    "twitter_api = twitter.Twitter(auth=auth)\n",
    "print(twitter_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of statuses 75\n",
      "Length of statuses 152\n",
      "Length of statuses 223\n",
      "Length of statuses 284\n",
      "Length of statuses 369\n",
      "Length of statuses 469\n",
      "Length of statuses 569\n",
      "Length of statuses 669\n",
      "Length of statuses 769\n",
      "Length of statuses 869\n",
      "Length of statuses 969\n",
      "Length of statuses 1069\n",
      "Length of statuses 1168\n",
      "Length of statuses 1268\n",
      "Length of statuses 1354\n",
      "Length of statuses 1452\n",
      "Length of statuses 1552\n",
      "Length of statuses 1652\n",
      "Length of statuses 1752\n",
      "Length of statuses 1852\n",
      "Length of statuses 1952\n",
      "Length of statuses 2052\n",
      "Length of statuses 2151\n",
      "Length of statuses 2251\n",
      "Length of statuses 2351\n",
      "Length of statuses 2451\n",
      "Length of statuses 2551\n",
      "Length of statuses 2638\n",
      "Length of statuses 2697\n",
      "Length of statuses 2758\n",
      "Length of statuses 2809\n",
      "Length of statuses 2906\n",
      "Length of statuses 3006\n",
      "Length of statuses 3106\n",
      "Length of statuses 3206\n",
      "Length of statuses 3306\n",
      "Length of statuses 3399\n",
      "Length of statuses 3499\n",
      "Length of statuses 3599\n",
      "Length of statuses 3699\n",
      "Length of statuses 3796\n",
      "Length of statuses 3896\n",
      "Length of statuses 3996\n",
      "Length of statuses 4096\n",
      "Length of statuses 4195\n",
      "Length of statuses 4295\n",
      "Length of statuses 4395\n",
      "Length of statuses 4495\n",
      "Length of statuses 4595\n",
      "Length of statuses 4695\n",
      "Length of statuses 4795\n",
      "Length of statuses 4895\n",
      "Length of statuses 4960\n",
      "Length of statuses 5015\n",
      "Length of statuses 5058\n",
      "Length of statuses 5158\n",
      "Length of statuses 5258\n",
      "Length of statuses 5347\n",
      "Length of statuses 5447\n",
      "Length of statuses 5547\n",
      "Length of statuses 5647\n",
      "Length of statuses 5742\n",
      "Length of statuses 5842\n",
      "Length of statuses 5942\n",
      "Length of statuses 6031\n",
      "Length of statuses 6119\n",
      "Length of statuses 6219\n",
      "Length of statuses 6319\n",
      "Length of statuses 6419\n",
      "Length of statuses 6519\n",
      "Length of statuses 6607\n",
      "Length of statuses 6707\n",
      "Length of statuses 6807\n",
      "Length of statuses 6905\n",
      "Length of statuses 7005\n",
      "Length of statuses 7105\n",
      "Length of statuses 7205\n",
      "Length of statuses 7305\n",
      "Length of statuses 7402\n",
      "Length of statuses 7468\n",
      "Length of statuses 7527\n",
      "Length of statuses 7580\n",
      "Length of statuses 7653\n",
      "Length of statuses 7753\n",
      "Length of statuses 7848\n",
      "Length of statuses 7948\n",
      "Length of statuses 8048\n",
      "Length of statuses 8148\n",
      "Length of statuses 8248\n",
      "Length of statuses 8344\n",
      "Length of statuses 8444\n",
      "Length of statuses 8544\n",
      "Length of statuses 8641\n",
      "Length of statuses 8740\n",
      "Length of statuses 8840\n",
      "Length of statuses 8937\n",
      "Length of statuses 9034\n",
      "Length of statuses 9134\n",
      "Length of statuses 9234\n",
      "Length of statuses 9330\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import unquote\n",
    "import json\n",
    "\n",
    "q = '#AI' \n",
    "\n",
    "count = 500\n",
    "\n",
    "search_results = twitter_api.search.tweets(q=q, count=count)\n",
    "statuses = search_results['statuses']\n",
    "#print(len(statuses))\n",
    "# Iterate through 5 more batches of results by following the cursor\n",
    "for _ in range(100):\n",
    "    print(\"Length of statuses\", len(statuses))\n",
    "    try:\n",
    "        next_results = search_results['search_metadata']['next_results']\n",
    "    except KeyError(e): # No more results when next_results doesn't exist\n",
    "        break\n",
    "    \n",
    "    kwargs = dict([ kv.split('=') for kv in unquote(next_results[1:]).split(\"&\") ])\n",
    "    \n",
    "    search_results = twitter_api.search.tweets(**kwargs)\n",
    "    statuses += search_results['statuses']\n",
    "\n",
    "# Show one sample search result by slicing the list...\n",
    "#print(json.dumps(statuses[0], indent=1))\n",
    "\n",
    "with open('data.json', 'w') as f:\n",
    "    json.dump(statuses, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9428\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "statuses = json.loads(open('data.json').read())\n",
    "#print(len(statuses))\n",
    "\n",
    "t = [ status for status in statuses]# if status['id'] == 1218698058164310023 ][0]\n",
    "# Explore the variable t to get familiarized with the data structure...\n",
    "print(len(t))\n",
    "#print(t['retweet_count'])\n",
    "#print(t['retweeted_status'])\n",
    "#print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tweets = pd.DataFrame()\n",
    "tweets['text'] = list(map(lambda tweet: tweet['text'], statuses))\n",
    "tweets['lang'] = list(map(lambda tweet: tweet['lang'], statuses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(tweets['text'])-1):\n",
    "    temp = tweets['text'][i].split(':')\n",
    "    if(len(temp)>1):\n",
    "        tweets['text'][i] = temp[1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        #quantumdpi moves the #cloudfoundry! #mobilit...\n",
       "1        The convergence of #AI, #ML &amp; #RoboticPro...\n",
       "2        #AI isnt just about smart cities, autonomous ...\n",
       "3        The collaboration between @UCLA &amp; USC Sig...\n",
       "4        #quantumdpi moves the #cloudfoundry! #mobilit...\n",
       "                              ...                        \n",
       "9423     This might be the most expressive robot youâ€™v...\n",
       "9424     #ArtificialIntelligence Is A #Tool Or #Weapon...\n",
       "9425    What do you call a dead rude country watching ...\n",
       "9426     #Michigan will be more impacted by #AI and #a...\n",
       "9427    RT @ipfconline1: #BigData and the Future of #S...\n",
       "Name: text, Length: 9428, dtype: object"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_csv(\"AIData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"AIData.csv\",engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tok = WordPunctTokenizer()\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "patt = r'(\\s*)https(\\s*)'\n",
    "patt2 = r'\\\\(\\s*)'\n",
    "\n",
    "combined_pat = r'|'.join((pat1, pat2,patt,patt2))\n",
    "def tweet_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    \n",
    "    # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
    "    # I will tokenize and join together to remove unneccessary white spaces\n",
    "    words = tok.tokenize(lower_case)\n",
    "    return (\" \".join(words)).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweet_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(data['text'])):                                                                 \n",
    "    clean_tweet_texts.append(tweet_cleaner(data['text'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedData = pd.DataFrame(clean_tweet_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(cleanedData[0])):\n",
    "    if (len(cleanedData[0][i])<20):\n",
    "        cleanedData[0][i]=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedData.to_csv(\"CleanedData2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"t co ej ltedunh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
